{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5\n",
    "\n",
    "\n",
    "Go over the lecture notes. You will perform to tasks for this homework\n",
    "\n",
    "1. Use analytical gradient to find function minimum point\n",
    "2. Use Gradient Descent to Fit a linear model to data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Find the maximum of the equation withing <0.001 error\n",
    "\\begin{equation}\n",
    "z=4-x^2-y^2\n",
    "\\end{equation}\n",
    "\n",
    "#### repeat lecture cell 4-7 .\n",
    "\n",
    "#### be carefull to adjust the direction and tune the step size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True maximum point:  [0, 0, 0]\n",
      "Starting iteration v:  [2.7885359691576745, -9.49978489554666]  distance to maximum: 9.900598260354906\n",
      "Iteration:  0 current v: [2.2308287753261395, -7.599827916437329]\n",
      "distance: 7.920478608283926\n",
      "Iteration:  10 current v: [0.2395334158250376, -0.8160253089081537]\n",
      "distance: 0.8504549147811765\n",
      "Iteration:  20 current v: [0.02571970468169264, -0.08762005036172045]\n",
      "distance: 0.0913169011426905\n",
      "Iteration:  30 current v: [0.0027616322617661994, -0.009408131269436557]\n",
      "distance: 0.009805077599498018\n",
      "Iteration:  40 current v: [0.00029652800619660833, -0.0010101904029676243]\n",
      "distance: 0.0010528121906146542\n",
      "Iteration:  50 current v: [3.183945222406294e-05, -0.0001084683685869752]\n",
      "distance: 0.00011304484818800145\n",
      "Iteration:  60 current v: [3.4187351506226194e-06, -1.1646702393288302e-05]\n",
      "distance: 1.2138098148718775e-05\n",
      "Iteration:  70 current v: [3.670838916402447e-07, -1.250555147135455e-06]\n",
      "distance: 1.3033183646096323e-06\n",
      "Iteration:  80 current v: [3.941533273708148e-08, -1.3427733646978115e-07]\n",
      "distance: 1.3994274380686433e-07\n",
      "Iteration:  90 current v: [4.232189126668078e-09, -1.4417919218292457e-08]\n",
      "distance: 1.5026237699074725e-08\n",
      "Maximum point:  [5.680348090476936e-10, -1.9351403599667498e-09]\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "# lets define some supporting functions distance, add, scalar_multiply\n",
    "from math import sqrt\n",
    "def scalar_multiply(s, v):\n",
    "    return [s*v_i for v_i in v]\n",
    "\n",
    "def add(v,w):\n",
    "    return [v_i+w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "def distance(v,w):\n",
    "    return sqrt(sum([(v_i-w_i)**2 for v_i,w_i in zip(v,w)]))\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def grad_func1(v):\n",
    "    return [-2 * v_i for v_i in v]\n",
    "\n",
    "\n",
    "\n",
    "# pick a random starting point\n",
    "random.seed(42)\n",
    "v = [random.uniform(-10, 10) for i in range(2)]\n",
    "print(\"True maximum point: \", [0,0,0])\n",
    "print(\"Starting iteration v: \",v,\" distance to maximum:\",distance(v,[0,0,0]))\n",
    "for epoch in range(100):\n",
    "    grad = grad_func1(v)    # compute the gradient at v\n",
    "    v = gradient_step(v, grad, +0.1)    # take a positive gradient step. WE ARE SEEKING MAXIMUM\n",
    "    if epoch%10==0:\n",
    "        print(\"Iteration: \",epoch, \"current v:\", v)\n",
    "        print(\"distance:\",distance(v,[0,0,0]))\n",
    "    \n",
    "    \n",
    "print(\"Maximum point: \", v)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Use gradient descent to fit a linear model to data\n",
    "\n",
    "The hidden function is \n",
    "\\begin{equation}\n",
    "t = 3x+4y-5 \\quad -1< x < 1 \\;, -1< y < 1 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "you will generate inputs and outputs\n",
    "\n",
    "then you will try to fit to a model  in the form z = ax+by+c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 20) (20, 20)\n",
      "400\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "x = np.arange(-1, 1, 0.1)\n",
    "y = np.arange(-1, 1, 0.1)\n",
    "xx, yy = np.meshgrid(x, y, sparse=False)\n",
    "print(xx.shape, yy.shape)\n",
    "xx1 = xx.flatten()\n",
    "yy1 = yy.flatten()\n",
    "\n",
    "def f(x,y):\n",
    "    t = 3*x+4*y-5\n",
    "    return t\n",
    "# use zip(xx,yy) for all points in the grid\n",
    "\n",
    "targets = f(xx1,yy1) #[f(x,y) for x,y in zip(xx,yy)]\n",
    "print(len(targets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.20000000e+01, -1.17000000e+01, -1.14000000e+01, -1.11000000e+01,\n",
       "       -1.08000000e+01, -1.05000000e+01, -1.02000000e+01, -9.90000000e+00,\n",
       "       -9.60000000e+00, -9.30000000e+00, -9.00000000e+00, -8.70000000e+00,\n",
       "       -8.40000000e+00, -8.10000000e+00, -7.80000000e+00, -7.50000000e+00,\n",
       "       -7.20000000e+00, -6.90000000e+00, -6.60000000e+00, -6.30000000e+00,\n",
       "       -1.16000000e+01, -1.13000000e+01, -1.10000000e+01, -1.07000000e+01,\n",
       "       -1.04000000e+01, -1.01000000e+01, -9.80000000e+00, -9.50000000e+00,\n",
       "       -9.20000000e+00, -8.90000000e+00, -8.60000000e+00, -8.30000000e+00,\n",
       "       -8.00000000e+00, -7.70000000e+00, -7.40000000e+00, -7.10000000e+00,\n",
       "       -6.80000000e+00, -6.50000000e+00, -6.20000000e+00, -5.90000000e+00,\n",
       "       -1.12000000e+01, -1.09000000e+01, -1.06000000e+01, -1.03000000e+01,\n",
       "       -1.00000000e+01, -9.70000000e+00, -9.40000000e+00, -9.10000000e+00,\n",
       "       -8.80000000e+00, -8.50000000e+00, -8.20000000e+00, -7.90000000e+00,\n",
       "       -7.60000000e+00, -7.30000000e+00, -7.00000000e+00, -6.70000000e+00,\n",
       "       -6.40000000e+00, -6.10000000e+00, -5.80000000e+00, -5.50000000e+00,\n",
       "       -1.08000000e+01, -1.05000000e+01, -1.02000000e+01, -9.90000000e+00,\n",
       "       -9.60000000e+00, -9.30000000e+00, -9.00000000e+00, -8.70000000e+00,\n",
       "       -8.40000000e+00, -8.10000000e+00, -7.80000000e+00, -7.50000000e+00,\n",
       "       -7.20000000e+00, -6.90000000e+00, -6.60000000e+00, -6.30000000e+00,\n",
       "       -6.00000000e+00, -5.70000000e+00, -5.40000000e+00, -5.10000000e+00,\n",
       "       -1.04000000e+01, -1.01000000e+01, -9.80000000e+00, -9.50000000e+00,\n",
       "       -9.20000000e+00, -8.90000000e+00, -8.60000000e+00, -8.30000000e+00,\n",
       "       -8.00000000e+00, -7.70000000e+00, -7.40000000e+00, -7.10000000e+00,\n",
       "       -6.80000000e+00, -6.50000000e+00, -6.20000000e+00, -5.90000000e+00,\n",
       "       -5.60000000e+00, -5.30000000e+00, -5.00000000e+00, -4.70000000e+00,\n",
       "       -1.00000000e+01, -9.70000000e+00, -9.40000000e+00, -9.10000000e+00,\n",
       "       -8.80000000e+00, -8.50000000e+00, -8.20000000e+00, -7.90000000e+00,\n",
       "       -7.60000000e+00, -7.30000000e+00, -7.00000000e+00, -6.70000000e+00,\n",
       "       -6.40000000e+00, -6.10000000e+00, -5.80000000e+00, -5.50000000e+00,\n",
       "       -5.20000000e+00, -4.90000000e+00, -4.60000000e+00, -4.30000000e+00,\n",
       "       -9.60000000e+00, -9.30000000e+00, -9.00000000e+00, -8.70000000e+00,\n",
       "       -8.40000000e+00, -8.10000000e+00, -7.80000000e+00, -7.50000000e+00,\n",
       "       -7.20000000e+00, -6.90000000e+00, -6.60000000e+00, -6.30000000e+00,\n",
       "       -6.00000000e+00, -5.70000000e+00, -5.40000000e+00, -5.10000000e+00,\n",
       "       -4.80000000e+00, -4.50000000e+00, -4.20000000e+00, -3.90000000e+00,\n",
       "       -9.20000000e+00, -8.90000000e+00, -8.60000000e+00, -8.30000000e+00,\n",
       "       -8.00000000e+00, -7.70000000e+00, -7.40000000e+00, -7.10000000e+00,\n",
       "       -6.80000000e+00, -6.50000000e+00, -6.20000000e+00, -5.90000000e+00,\n",
       "       -5.60000000e+00, -5.30000000e+00, -5.00000000e+00, -4.70000000e+00,\n",
       "       -4.40000000e+00, -4.10000000e+00, -3.80000000e+00, -3.50000000e+00,\n",
       "       -8.80000000e+00, -8.50000000e+00, -8.20000000e+00, -7.90000000e+00,\n",
       "       -7.60000000e+00, -7.30000000e+00, -7.00000000e+00, -6.70000000e+00,\n",
       "       -6.40000000e+00, -6.10000000e+00, -5.80000000e+00, -5.50000000e+00,\n",
       "       -5.20000000e+00, -4.90000000e+00, -4.60000000e+00, -4.30000000e+00,\n",
       "       -4.00000000e+00, -3.70000000e+00, -3.40000000e+00, -3.10000000e+00,\n",
       "       -8.40000000e+00, -8.10000000e+00, -7.80000000e+00, -7.50000000e+00,\n",
       "       -7.20000000e+00, -6.90000000e+00, -6.60000000e+00, -6.30000000e+00,\n",
       "       -6.00000000e+00, -5.70000000e+00, -5.40000000e+00, -5.10000000e+00,\n",
       "       -4.80000000e+00, -4.50000000e+00, -4.20000000e+00, -3.90000000e+00,\n",
       "       -3.60000000e+00, -3.30000000e+00, -3.00000000e+00, -2.70000000e+00,\n",
       "       -8.00000000e+00, -7.70000000e+00, -7.40000000e+00, -7.10000000e+00,\n",
       "       -6.80000000e+00, -6.50000000e+00, -6.20000000e+00, -5.90000000e+00,\n",
       "       -5.60000000e+00, -5.30000000e+00, -5.00000000e+00, -4.70000000e+00,\n",
       "       -4.40000000e+00, -4.10000000e+00, -3.80000000e+00, -3.50000000e+00,\n",
       "       -3.20000000e+00, -2.90000000e+00, -2.60000000e+00, -2.30000000e+00,\n",
       "       -7.60000000e+00, -7.30000000e+00, -7.00000000e+00, -6.70000000e+00,\n",
       "       -6.40000000e+00, -6.10000000e+00, -5.80000000e+00, -5.50000000e+00,\n",
       "       -5.20000000e+00, -4.90000000e+00, -4.60000000e+00, -4.30000000e+00,\n",
       "       -4.00000000e+00, -3.70000000e+00, -3.40000000e+00, -3.10000000e+00,\n",
       "       -2.80000000e+00, -2.50000000e+00, -2.20000000e+00, -1.90000000e+00,\n",
       "       -7.20000000e+00, -6.90000000e+00, -6.60000000e+00, -6.30000000e+00,\n",
       "       -6.00000000e+00, -5.70000000e+00, -5.40000000e+00, -5.10000000e+00,\n",
       "       -4.80000000e+00, -4.50000000e+00, -4.20000000e+00, -3.90000000e+00,\n",
       "       -3.60000000e+00, -3.30000000e+00, -3.00000000e+00, -2.70000000e+00,\n",
       "       -2.40000000e+00, -2.10000000e+00, -1.80000000e+00, -1.50000000e+00,\n",
       "       -6.80000000e+00, -6.50000000e+00, -6.20000000e+00, -5.90000000e+00,\n",
       "       -5.60000000e+00, -5.30000000e+00, -5.00000000e+00, -4.70000000e+00,\n",
       "       -4.40000000e+00, -4.10000000e+00, -3.80000000e+00, -3.50000000e+00,\n",
       "       -3.20000000e+00, -2.90000000e+00, -2.60000000e+00, -2.30000000e+00,\n",
       "       -2.00000000e+00, -1.70000000e+00, -1.40000000e+00, -1.10000000e+00,\n",
       "       -6.40000000e+00, -6.10000000e+00, -5.80000000e+00, -5.50000000e+00,\n",
       "       -5.20000000e+00, -4.90000000e+00, -4.60000000e+00, -4.30000000e+00,\n",
       "       -4.00000000e+00, -3.70000000e+00, -3.40000000e+00, -3.10000000e+00,\n",
       "       -2.80000000e+00, -2.50000000e+00, -2.20000000e+00, -1.90000000e+00,\n",
       "       -1.60000000e+00, -1.30000000e+00, -1.00000000e+00, -7.00000000e-01,\n",
       "       -6.00000000e+00, -5.70000000e+00, -5.40000000e+00, -5.10000000e+00,\n",
       "       -4.80000000e+00, -4.50000000e+00, -4.20000000e+00, -3.90000000e+00,\n",
       "       -3.60000000e+00, -3.30000000e+00, -3.00000000e+00, -2.70000000e+00,\n",
       "       -2.40000000e+00, -2.10000000e+00, -1.80000000e+00, -1.50000000e+00,\n",
       "       -1.20000000e+00, -9.00000000e-01, -6.00000000e-01, -3.00000000e-01,\n",
       "       -5.60000000e+00, -5.30000000e+00, -5.00000000e+00, -4.70000000e+00,\n",
       "       -4.40000000e+00, -4.10000000e+00, -3.80000000e+00, -3.50000000e+00,\n",
       "       -3.20000000e+00, -2.90000000e+00, -2.60000000e+00, -2.30000000e+00,\n",
       "       -2.00000000e+00, -1.70000000e+00, -1.40000000e+00, -1.10000000e+00,\n",
       "       -8.00000000e-01, -5.00000000e-01, -2.00000000e-01,  1.00000000e-01,\n",
       "       -5.20000000e+00, -4.90000000e+00, -4.60000000e+00, -4.30000000e+00,\n",
       "       -4.00000000e+00, -3.70000000e+00, -3.40000000e+00, -3.10000000e+00,\n",
       "       -2.80000000e+00, -2.50000000e+00, -2.20000000e+00, -1.90000000e+00,\n",
       "       -1.60000000e+00, -1.30000000e+00, -1.00000000e+00, -7.00000000e-01,\n",
       "       -4.00000000e-01, -1.00000000e-01,  2.00000000e-01,  5.00000000e-01,\n",
       "       -4.80000000e+00, -4.50000000e+00, -4.20000000e+00, -3.90000000e+00,\n",
       "       -3.60000000e+00, -3.30000000e+00, -3.00000000e+00, -2.70000000e+00,\n",
       "       -2.40000000e+00, -2.10000000e+00, -1.80000000e+00, -1.50000000e+00,\n",
       "       -1.20000000e+00, -9.00000000e-01, -6.00000000e-01, -3.00000000e-01,\n",
       "       -2.66453526e-15,  3.00000000e-01,  6.00000000e-01,  9.00000000e-01,\n",
       "       -4.40000000e+00, -4.10000000e+00, -3.80000000e+00, -3.50000000e+00,\n",
       "       -3.20000000e+00, -2.90000000e+00, -2.60000000e+00, -2.30000000e+00,\n",
       "       -2.00000000e+00, -1.70000000e+00, -1.40000000e+00, -1.10000000e+00,\n",
       "       -8.00000000e-01, -5.00000000e-01, -2.00000000e-01,  1.00000000e-01,\n",
       "        4.00000000e-01,  7.00000000e-01,  1.00000000e+00,  1.30000000e+00])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### your model \n",
    "\n",
    "\\begin{equation}\n",
    "z = ax+by+c\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_est(x,y,a,b,c):\n",
    "    z = a*x+b*y+c\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt and repeat lecture code 9 and 10 to solve a,b,c\n",
    "\n",
    "#### Error is defined in the same way, and its gradient is same\n",
    "\n",
    "#### make sure you tune the step size. \n",
    "\n",
    "#### Read the gradient descent chapter from the book if you are confused\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "zz = targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_gradient(x, y, z, a, b, c):\n",
    "    #print(x,y,z,a,b,c)\n",
    "    #return\n",
    "    predicted = a*x + b*y + c    # The prediction of the model.\n",
    "    error = (predicted - z)              # error is E=(predicted - actual)\n",
    "    squared_error = error ** 2           # We'll minimize squared error. E=(predicted - actual)^2\n",
    "    # grad is dE/da, dE/db, dE,b\n",
    "    grad = [2 * error * x, 2 * error * y, 2 * error]    # using its gradient. \n",
    "    return grad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start at (a,b,c):  (-0.24108911648470444, 0.9790467012731905, 0.2799995197081857)\n",
      "Iteration:  0 , grad(a,b,c): [-2.7146344265092086, -2.568244107700204, 11.186203280937525]\n",
      "\n",
      " Iteration:  0 current (a,b,c): (-0.24108911648470444, 0.9790467012731905, 0.2799995197081857) \n",
      "\n",
      "Iteration:  1 , grad(a,b,c): [-2.419607765069862, -2.282952402461663, 8.89613383940793]\n",
      "Iteration:  2 , grad(a,b,c): [-2.1673912302148715, -2.039823449220117, 7.069881469851024]\n",
      "Iteration:  3 , grad(a,b,c): [-1.9504572913673557, -1.8313727678087497, 5.613833029086471]\n",
      "Iteration:  4 , grad(a,b,c): [-1.7627226361709734, -1.6515572334290172, 4.4532481226774125]\n",
      "Iteration:  5 , grad(a,b,c): [-1.5992619597040274, -1.4954890562444092, 3.5284556994459315]\n",
      "Iteration:  6 , grad(a,b,c): [-1.456079106881278, -1.3592071015017249, 2.791817049397259]\n",
      "Iteration:  7 , grad(a,b,c): [-1.3299240326755084, -1.2394940156536949, 2.2053007774339792]\n",
      "Iteration:  8 , grad(a,b,c): [-1.218146367704082, -1.1337299468142203, 1.738546441463892]\n",
      "Iteration:  9 , grad(a,b,c): [-1.118578231679863, -1.0397755027791766, 1.3673183900259305]\n",
      "Iteration:  10 , grad(a,b,c): [-1.029440418505662, -0.9558780710768723, 1.072271174676154]\n",
      "\n",
      " Iteration:  10 current (a,b,c): (1.5325911883129988, 2.643211259634489, -4.605074290264549) \n",
      "\n",
      "Iteration:  11 , grad(a,b,c): [-0.9492672596834844, -0.8805968083587074, 0.8379637548450981]\n",
      "Iteration:  12 , grad(a,b,c): [-0.87684641733206, -0.8127425510203813, 0.6520723631956561]\n",
      "Iteration:  13 , grad(a,b,c): [-0.8111706124633458, -0.751329653261393, 0.5047620008730003]\n",
      "Iteration:  14 , grad(a,b,c): [-0.7513988965929409, -0.695537361177918, 0.3881845980411528]\n",
      "Iteration:  15 , grad(a,b,c): [-0.6968255558602132, -0.6446788125502892, 0.2960783158552128]\n",
      "Iteration:  16 , grad(a,b,c): [-0.6468551210527518, -0.5981761361729376, 0.22344760900006527]\n",
      "Iteration:  17 , grad(a,b,c): [-0.6009822637841294, -0.5555404313988241, 0.1663077746277945]\n",
      "Iteration:  18 , grad(a,b,c): [-0.5587756041486153, -0.5163556536169329, 0.1214809927504059]\n",
      "Iteration:  19 , grad(a,b,c): [-0.5198646509163461, -0.4802656270950202, 0.08643348162266926]\n",
      "Iteration:  20 , grad(a,b,c): [-0.48392925167517675, -0.44696356293796885, 0.059145482518020896]\n",
      "\n",
      " Iteration:  20 current (a,b,c): (2.2767338683469536, 3.3323213702074166, -5.039974496813271) \n",
      "\n",
      "Iteration:  21 , grad(a,b,c): [-0.45069105520629105, -0.41618358477010675, 0.038007457868285965]\n",
      "Iteration:  22 , grad(a,b,c): [-0.4199065881364019, -0.38769386448422444, 0.021737219894864324]\n",
      "Iteration:  23 , grad(a,b,c): [-0.39136162760007204, -0.3612910500707642, 0.009313771389684007]\n",
      "Iteration:  24 , grad(a,b,c): [-0.36486661531193443, -0.3367957311883263, -7.550966496129074e-05]\n",
      "Iteration:  25 , grad(a,b,c): [-0.34025290931709057, -0.3140487389877021, -0.007077031196970811]\n",
      "Iteration:  26 , grad(a,b,c): [-0.31736971033532135, -0.2929081173328378, -0.012204641440625113]\n",
      "Iteration:  27 , grad(a,b,c): [-0.29608153209859484, -0.273246635030776, -0.015866491429181343]\n",
      "Iteration:  28 , grad(a,b,c): [-0.2762661110447651, -0.2549497346319566, -0.01838647481463972]\n",
      "Iteration:  29 , grad(a,b,c): [-0.25781267148559633, -0.23791383410423966, -0.020021338308478062]\n",
      "Iteration:  30 , grad(a,b,c): [-0.24062047896209407, -0.2220449142665976, -0.020974335702680893]\n",
      "\n",
      " Iteration:  30 current (a,b,c): (2.636587675568078, 3.664520855561306, -5.0454317412948715) \n",
      "\n",
      "Iteration:  31 , grad(a,b,c): [-0.22459762777152698, -0.20725733812828115, -0.021406122494432366]\n",
      "Iteration:  32 , grad(a,b,c): [-0.20966001926671513, -0.19347285888474502, -0.02144344765454492]\n",
      "Iteration:  33 , grad(a,b,c): [-0.19573049602294842, -0.1806197818063792, -0.021186086905149653]\n",
      "Iteration:  34 , grad(a,b,c): [-0.1827381037675591, -0.1686322520463921, -0.02071237230241266]\n",
      "Iteration:  35 , grad(a,b,c): [-0.1706174584121333, -0.15744964583042417, -0.020083601400069116]\n",
      "Iteration:  36 , grad(a,b,c): [-0.15930819988960612, -0.1470160468445804, -0.019347552162481116]\n",
      "Iteration:  37 , grad(a,b,c): [-0.14875451799520498, -0.1372797931276733, -0.01854128419732676]\n",
      "Iteration:  38 , grad(a,b,c): [-0.13890473823493557, -0.1281930825710949, -0.017693370469091037]\n",
      "Iteration:  39 , grad(a,b,c): [-0.12971095793660023, -0.11971162737440513, -0.016825674583333197]\n",
      "Iteration:  40 , grad(a,b,c): [-0.12112872468699408, -0.1117943496071851, -0.015954765519776568]\n",
      "\n",
      " Iteration:  40 current (a,b,c): (2.8166519353940105, 3.8306885896493625, -5.02561035650772) \n",
      "\n",
      "Iteration:  41 , grad(a,b,c): [-0.1131167506133597, -0.10440311147635793, -0.015093043158763515]\n",
      "Iteration:  42 , grad(a,b,c): [-0.10563665719811423, -0.09750247506372305, -0.014249633147908452]\n",
      "Iteration:  43 , grad(a,b,c): [-0.09865274625978764, -0.09105948723733359, -0.01343109784094572]\n",
      "Iteration:  44 , grad(a,b,c): [-0.0921317934951726, -0.08504348619771186, -0.012642000607728408]\n",
      "Iteration:  45 , grad(a,b,c): [-0.08604286159397452, -0.0794259267317949, -0.011885353283111008]\n",
      "Iteration:  46 , grad(a,b,c): [-0.0803571304366433, -0.07418022174279891, -0.011162970509745757]\n",
      "Iteration:  47 , grad(a,b,c): [-0.07504774229161446, -0.06928159802591079, -0.01047574992959079]\n",
      "Iteration:  48 , grad(a,b,c): [-0.07008966025835903, -0.06470696458632486, -0.009823893346848593]\n",
      "Iteration:  49 , grad(a,b,c): [-0.06545953847222422, -0.06043479206238052, -0.009207080925925346]\n",
      "Iteration:  50 , grad(a,b,c): [-0.06113560280781309, -0.05644500203422401, -0.008624608046086762]\n",
      "\n",
      " Iteration:  50 current (a,b,c): (2.9074182959246353, 3.9144718309225137, -5.013217797680686) \n",
      "\n",
      "Iteration:  51 , grad(a,b,c): [-0.05709754099913332, -0.052718865176988025, -0.00807549248529038]\n",
      "Iteration:  52 , grad(a,b,c): [-0.05332640124445598, -0.04923890736448305, -0.0075585580499929455]\n",
      "Iteration:  53 , grad(a,b,c): [-0.04980449848789513, -0.045988822950940304, -0.007072499526083982]\n",
      "Iteration:  54 , grad(a,b,c): [-0.046515327672991436, -0.042953394559244185, -0.006615932835256058]\n",
      "Iteration:  55 , grad(a,b,c): [-0.04344348334997409, -0.0401184187882911, -0.006187433490527492]\n",
      "Iteration:  56 , grad(a,b,c): [-0.040574585091036924, -0.03747063732270588, -0.005785565813804585]\n",
      "Iteration:  57 , grad(a,b,c): [-0.037895208229414154, -0.03499767298767697, -0.005408904875181946]\n",
      "Iteration:  58 , grad(a,b,c): [-0.03539281949030137, -0.03268797034213971, -0.005056052712316006]\n",
      "Iteration:  59 , grad(a,b,c): [-0.033055717126403115, -0.030530740446594345, -0.00472565006817693]\n",
      "Iteration:  60 , grad(a,b,c): [-0.030872975209392528, -0.02851590947879114, -0.004416384630271786]\n",
      "\n",
      " Iteration:  60 current (a,b,c): (2.9532424143745772, 3.9567868741198424, -5.006706727890415) \n",
      "\n",
      "Iteration:  61 , grad(a,b,c): [-0.02883439176192646, -0.02663407090241011, -0.004126996551099906]\n",
      "Iteration:  62 , grad(a,b,c): [-0.026930440443937356, -0.024876440921578556, -0.0038562818675240006]\n",
      "Iteration:  63 , grad(a,b,c): [-0.025152225532407885, -0.023234816978286058, -0.00360309430767507]\n",
      "Iteration:  64 , grad(a,b,c): [-0.023491439956324106, -0.02170153907105137, -0.0033663458712463435]\n",
      "Iteration:  65 , grad(a,b,c): [-0.021940326168427293, -0.02026945369202513, -0.0031450064872702746]\n",
      "Iteration:  66 , grad(a,b,c): [-0.020491639653169306, -0.01893188019644809, -0.002938102988420175]\n",
      "Iteration:  67 , grad(a,b,c): [-0.019138614886193017, -0.01768257943334377, -0.0027447175892315735]\n",
      "Iteration:  68 , grad(a,b,c): [-0.01787493357499393, -0.016515724479759093, -0.002563986014579809]\n",
      "Iteration:  69 , grad(a,b,c): [-0.01669469502337511, -0.015425873332973449, -0.002395095392211668]\n",
      "Iteration:  70 , grad(a,b,c): [-0.015592388474064592, -0.01440794342607465, -0.0022372819973327162]\n",
      "\n",
      " Iteration:  70 current (a,b,c): (2.976384582595592, 3.9781657029685094, -5.003391126720461) \n",
      "\n",
      "Iteration:  71 , grad(a,b,c): [-0.014562867294562519, -0.013457187842264042, -0.0020898289168672824]\n",
      "Iteration:  72 , grad(a,b,c): [-0.013601324881074264, -0.012569173112353645, -0.0019520636848617645]\n",
      "Iteration:  73 , grad(a,b,c): [-0.012703272164334723, -0.011739758488233942, -0.0018233559278230428]\n",
      "Iteration:  74 , grad(a,b,c): [-0.011864516609358436, -0.010965076592718543, -0.0017031150487835701]\n",
      "Iteration:  75 , grad(a,b,c): [-0.011081142608722864, -0.010241515353189573, -0.0015907879710473783]\n",
      "Iteration:  76 , grad(a,b,c): [-0.010349493175972467, -0.009565701132931826, -0.001485856956456666]\n",
      "Iteration:  77 , grad(a,b,c): [-0.009666152852180402, -0.008934482980002179, -0.0013878375082535842]\n",
      "Iteration:  78 , grad(a,b,c): [-0.009027931744676945, -0.008344917918998601, -0.0012962763649239295]\n",
      "Iteration:  79 , grad(a,b,c): [-0.008431850622473315, -0.007794257216202569, -0.0012107495885759346]\n",
      "Iteration:  80 , grad(a,b,c): [-0.007875126998045138, -0.007279933553291536, -0.001130860749247109]\n",
      "\n",
      " Iteration:  80 current (a,b,c): (2.9880726766383345, 3.988967704374806, -5.0017134113239665) \n",
      "\n",
      "Iteration:  81 , grad(a,b,c): [-0.007355162129891876, -0.006799549049214523, -0.0010562392049108737]\n",
      "Iteration:  82 , grad(a,b,c): [-0.006869528884713423, -0.006350864073901501, -0.0009865384757202112]\n",
      "Iteration:  83 , grad(a,b,c): [-0.006415960402157917, -0.0059317868012649915, -0.0009214347101623144]\n",
      "Iteration:  84 , grad(a,b,c): [-0.005992339508914372, -0.005540363452480699, -0.0008606252401646697]\n",
      "Iteration:  85 , grad(a,b,c): [-0.005596688832492543, -0.005174769183811855, -0.0008038272217452169]\n",
      "Iteration:  86 , grad(a,b,c): [-0.00522716156834119, -0.004833299576297692, -0.0007507763575593041]\n",
      "Iteration:  87 , grad(a,b,c): [-0.004882032857049914, -0.004514362687477251, -0.0007012256974930887]\n",
      "Iteration:  88 , grad(a,b,c): [-0.004559691731258724, -0.004216471627962542, -0.0006549445134403209]\n",
      "Iteration:  89 , grad(a,b,c): [-0.004258633594584847, -0.003938237628157768, -0.0006117172443444874]\n",
      "Iteration:  90 , grad(a,b,c): [-0.0039774531973770875, -0.003678363562717452, -0.0005713425077031809]\n",
      "\n",
      " Iteration:  90 current (a,b,c): (2.9939759092890794, 3.9944256681381916, -5.000865592382488) \n",
      "\n",
      "Iteration:  91 , grad(a,b,c): [-0.0037148380764483804, -0.0034356379024939056, -0.0005336321737632366]\n",
      "Iteration:  92 , grad(a,b,c): [-0.003469562428112587, -0.003208929065726045, -0.0004984104987997174]\n",
      "Iteration:  93 , grad(a,b,c): [-0.0032404813858843196, -0.002997180142096406, -0.00046551331397769234]\n",
      "Iteration:  94 , grad(a,b,c): [-0.0030265256760988513, -0.0027994039650228726, -0.00043478726646213996]\n",
      "Iteration:  95 , grad(a,b,c): [-0.0028266966264825174, -0.002614678509192883, -0.0004060891095803232]\n",
      "Iteration:  96 , grad(a,b,c): [-0.00264006150434945, -0.002442142591859511, -0.0003792850390203717]\n",
      "Iteration:  97 , grad(a,b,c): [-0.002465749162652322, -0.002280991857843007, -0.00035425007217837034]\n",
      "Iteration:  98 , grad(a,b,c): [-0.0023029459735472786, -0.002130475029507891, -0.0003308674679484502]\n",
      "Iteration:  99 , grad(a,b,c): [-0.002150892030484324, -0.0019898904042234666, -0.00030902818438951487]\n",
      "Final Iteration: (a,b,c): (2.9967423406921747, 3.9969844484008377, -5.000468174637544)\n"
     ]
    }
   ],
   "source": [
    "def vector_sum(vectors):\n",
    "    num_elements = len(vectors[0])\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "    \n",
    "# Start with random values for slope and intercept.\n",
    "a = random.uniform(-1, 1) \n",
    "b = random.uniform(-1, 1)\n",
    "c = random.uniform(-1, 1)\n",
    "\n",
    "v = (a,b,c)\n",
    "print(\"Start at (a,b,c): \", v)\n",
    "learning_rate = 0.1\n",
    "\n",
    "for epoch in range(100):\n",
    "    # Compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(xi, yi,zi, a,b,c) for xi,yi,zi in zip (xx1,yy1,zz)] )\n",
    "    # Take a step in that direction\n",
    "    print(\"Iteration: \",epoch,\", grad(a,b,c):\",grad)\n",
    "    v = (a,b,c)\n",
    "    a,b,c = gradient_step(v, grad, -learning_rate)\n",
    "    if epoch%10==0:\n",
    "        print(\"\\n Iteration: \",epoch, \"current (a,b,c):\", v,\"\\n\")\n",
    "\n",
    "print(\"Final Iteration: (a,b,c):\", v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were seeking \n",
    "\\begin{equation}\n",
    "t = 3x+4y-5 \\quad -1< x < 1 \\;, -1< y < 1 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "After 100 iteration with learning rate 0.1 we have approximated a,b,c,\n",
    "\n",
    "Final Iteration: (a,b,c): (2.9959421865558644, 3.995178915591644, -5.000662639792767)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
