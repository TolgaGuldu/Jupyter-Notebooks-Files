{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 5\n",
    "\n",
    "\n",
    "Go over the lecture notes. You will perform to tasks for this homework\n",
    "\n",
    "1. Use analytical gradient to find function minimum point\n",
    "2. Use Gradient Descent to Fit a linear model to data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1 Find the maximum of the equation withing <0.001 error\n",
    "\\begin{equation}\n",
    "z=4-x^2-y^2\n",
    "\\end{equation}\n",
    "\n",
    "#### repeat lecture cell 4-7 .\n",
    "\n",
    "#### be carefull to adjust the direction and tune the step size. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write code here. \n",
    "\n",
    "\n",
    "\n",
    "## Partial derivatives\n",
    "\n",
    "#One parameter changing the others are fixed\n",
    "\n",
    "def partial_difference_quotient(f, vec, ix, h):\n",
    "    \"\"\"Returns the i-th partial difference quotient of f at v\"\"\"\n",
    "     # add h to just the ith element of v\n",
    "    #[x,y+h,z]\n",
    "    w = vec.copy()\n",
    "    w[ix] = w[ix]+h\n",
    "    #w = [v_j + (h if j == ix else 0) for j, v_j in enumerate(vec)]\n",
    "    return (f(w) - f(vec)) / h\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def estimate_gradient(f, vec, h=0.01):\n",
    "    # for vec we calculate df/dx, df/dy, ...\n",
    "    return [partial_difference_quotient(f, vec, ix, h)\n",
    "            for ix in range(len(vec))]\n",
    "\n",
    "\n",
    "import numpy as np \n",
    "points = [np.arange(-10, 11),np.arange(-10, 11)]\n",
    "print(\"Points:\",points)\n",
    "func_value = [sum_of_squares(p) for p in points]\n",
    "estimates = [estimate_gradient(sum_of_squares, p, h=0.2) for p in points]\n",
    "\n",
    "print(func_value)\n",
    "print(estimates)\n",
    "print(points)\n",
    "\n",
    "import random\n",
    "# lets define some supporting functions distance, add, scalar_multiply\n",
    "from math import sqrt\n",
    "def scalar_multiply(s, v):\n",
    "    return [s*v_i for v_i in v]\n",
    "\n",
    "def add(v,w):\n",
    "    return [v_i+w_i for v_i,w_i in zip(v,w)]\n",
    "\n",
    "def distance(v,w):\n",
    "    return sqrt(sum([(v_i-w_i)**2 for v_i,w_i in zip(v,w)]))\n",
    "\n",
    "def gradient_step(v, gradient, step_size):\n",
    "    \"\"\"Moves `step_size` in the `gradient` direction from `v`\"\"\"\n",
    "    assert len(v) == len(gradient)\n",
    "    step = scalar_multiply(step_size, gradient)\n",
    "    return add(v, step)\n",
    "\n",
    "def sum_of_squares_gradient(v):\n",
    "    return [2 * v_i for v_i in v]\n",
    "\n",
    "# pick a random starting point\n",
    "random.seed(42)\n",
    "v = [random.uniform(-10, 10) for i in range(2)]\n",
    "print(\"at v\",v, \" distance:\",distance(v,[0,0,0]))\n",
    "for epoch in range(100):\n",
    "    grad = sum_of_squares_gradient(v)    # compute the gradient at v\n",
    "    v = gradient_step(v, grad, -0.1)    # take a negative gradient step\n",
    "    print(epoch, v)\n",
    "    print(\"distance:\",distance(v,[0,0,0]))\n",
    "\n",
    "#assert distance(v, [0, 0, 0]) < 0.001    # v should be close to 0\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2 Use gradient descent to fit a linear model to data\n",
    "\n",
    "The hidden function is \n",
    "\\begin{equation}\n",
    "t = 3x+4y-5 \\quad -1< x < 1 \\;, -1< y < 1 \\\\\n",
    "\\end{equation}\n",
    "\n",
    "you will generate inputs and outputs\n",
    "\n",
    "then you will try to fit to a model  in the form z = ax+by+c\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "x = np.arange(-1, 1, 0.01)\n",
    "y = np.arange(-1, 1, 0.01)\n",
    "xx, yy = np.meshgrid(x, y, sparse=True)\n",
    "\n",
    "def f(x,y):\n",
    "    t = 3*x+4*y-5\n",
    "    return t\n",
    "# use zip(xx,yy) for all points in the grid\n",
    "\n",
    "targets = [f(x,y) for x,y in zip(xx,yy)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([ 4.  ,  3.97,  3.94,  3.91,  3.88,  3.85,  3.82,  3.79,  3.76,\n",
       "         3.73,  3.7 ,  3.67,  3.64,  3.61,  3.58,  3.55,  3.52,  3.49,\n",
       "         3.46,  3.43,  3.4 ,  3.37,  3.34,  3.31,  3.28,  3.25,  3.22,\n",
       "         3.19,  3.16,  3.13,  3.1 ,  3.07,  3.04,  3.01,  2.98,  2.95,\n",
       "         2.92,  2.89,  2.86,  2.83,  2.8 ,  2.77,  2.74,  2.71,  2.68,\n",
       "         2.65,  2.62,  2.59,  2.56,  2.53,  2.5 ,  2.47,  2.44,  2.41,\n",
       "         2.38,  2.35,  2.32,  2.29,  2.26,  2.23,  2.2 ,  2.17,  2.14,\n",
       "         2.11,  2.08,  2.05,  2.02,  1.99,  1.96,  1.93,  1.9 ,  1.87,\n",
       "         1.84,  1.81,  1.78,  1.75,  1.72,  1.69,  1.66,  1.63,  1.6 ,\n",
       "         1.57,  1.54,  1.51,  1.48,  1.45,  1.42,  1.39,  1.36,  1.33,\n",
       "         1.3 ,  1.27,  1.24,  1.21,  1.18,  1.15,  1.12,  1.09,  1.06,\n",
       "         1.03,  1.  ,  0.97,  0.94,  0.91,  0.88,  0.85,  0.82,  0.79,\n",
       "         0.76,  0.73,  0.7 ,  0.67,  0.64,  0.61,  0.58,  0.55,  0.52,\n",
       "         0.49,  0.46,  0.43,  0.4 ,  0.37,  0.34,  0.31,  0.28,  0.25,\n",
       "         0.22,  0.19,  0.16,  0.13,  0.1 ,  0.07,  0.04,  0.01, -0.02,\n",
       "        -0.05, -0.08, -0.11, -0.14, -0.17, -0.2 , -0.23, -0.26, -0.29,\n",
       "        -0.32, -0.35, -0.38, -0.41, -0.44, -0.47, -0.5 , -0.53, -0.56,\n",
       "        -0.59, -0.62, -0.65, -0.68, -0.71, -0.74, -0.77, -0.8 , -0.83,\n",
       "        -0.86, -0.89, -0.92, -0.95, -0.98, -1.01, -1.04, -1.07, -1.1 ,\n",
       "        -1.13, -1.16, -1.19, -1.22, -1.25, -1.28, -1.31, -1.34, -1.37,\n",
       "        -1.4 , -1.43, -1.46, -1.49, -1.52, -1.55, -1.58, -1.61, -1.64,\n",
       "        -1.67, -1.7 , -1.73, -1.76, -1.79, -1.82, -1.85, -1.88, -1.91,\n",
       "        -1.94, -1.97])]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### your model \n",
    "\n",
    "\\begin{equation}\n",
    "z = ax+by+c\n",
    "\\end{equation}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_est(x,y,a,b,c):\n",
    "    z = a*x+b*y+c\n",
    "    return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adapt and repeat lecture code 9 and 10 to find a,b,c\n",
    "\n",
    "#### Error is defined in the same way, and its gradient is same\n",
    "\n",
    "#### make sure you tune the step size. \n",
    "\n",
    "#### Read the gradient descent chapter from the book if you are confused\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# now we assume a linear model\n",
    "#y = mx+b  m is slope b is incercept\n",
    "\n",
    "# function we want to find minimum is prediction error\n",
    "# Error = (y-mx+b)**2\n",
    "\n",
    "# lets define gradient for x and y \n",
    "def linear_gradient(x, y, m, b):\n",
    "    \n",
    "    predicted = m * x + b    # The prediction of the model.\n",
    "    error = (predicted - y)              # error is (predicted - actual)\n",
    "    squared_error = error ** 2           # We'll minimize squared error\n",
    "    grad = [2 * error * x, 2 * error]    # using its gradient.\n",
    "    # 2(mx+b).x, 2(mx+b)\n",
    "    return grad\n",
    "\n",
    "\n",
    "def vector_sum(vectors):\n",
    "    num_elements = len(vectors[0])\n",
    "    return [sum(vector[i] for vector in vectors)\n",
    "            for i in range(num_elements)]\n",
    "\n",
    "def vector_mean(vectors):\n",
    "    \"\"\"Computes the element-wise average\"\"\"\n",
    "    n = len(vectors)\n",
    "    return scalar_multiply(1/n, vector_sum(vectors))\n",
    "    \n",
    "# Start with random values for slope and intercept.\n",
    "m,b = random.uniform(-1, 1), random.uniform(-1, 1)\n",
    "v = (m,b)\n",
    "print(\"starting point\", v)\n",
    "learning_rate = 0.001\n",
    "\n",
    "for epoch in range(5000):\n",
    "    # Compute the mean of the gradients\n",
    "    grad = vector_mean([linear_gradient(x, y, m,b) for x, y in inputs])\n",
    "    # Take a step in that direction\n",
    "    print(\"grad:\",grad)\n",
    "    v = (m,b)\n",
    "    m,b = gradient_step(v, grad, -learning_rate)\n",
    "    print(epoch, v)\n",
    "\n",
    "\n",
    "assert 19.9 < m < 20.1,   \"slope should be about 20\"\n",
    "assert 4.9 < b < 5.1, \"intercept should be about 5\"\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
